{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "from script.tool import ROOT, ROOT_NFS_TEST, ROOT_NFS_DATA, standardize_feature\n",
    "from script.func_extract_feature import convert_feature\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = ROOT_NFS_DATA / 'Cosmenet_products_15000/raw_data'\n",
    "device = torch.device(\"cuda:0\")\n",
    "df_pd = pd.read_csv(path_dataset / 'data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.func_extract_feature import select_timm_model, pipeline_timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 26.701688766479492 ms\n",
      "Output shape at layer : torch.Size([1, 1280])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_timm_model('efficientnet_b1', num_classes=0, pretrain=True)\n",
    "eff_pipe = pipeline_timm(device=device)\n",
    "eff_pipe.selct_model(model, preprocess)\n",
    "eff_pipe.report_test()\n",
    "cvt_feature_eff = convert_feature(eff_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_eff(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"test\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 40.941715240478516 ms\n",
      "Output shape at layer : torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_timm_model('efficientnet_b5', num_classes=0, pretrain=True)\n",
    "eff_b5_pipe = pipeline_timm(device=device)\n",
    "eff_b5_pipe.selct_model(model, preprocess)\n",
    "eff_b5_pipe.report_test()\n",
    "cvt_feature_eff_b5 = convert_feature(eff_b5_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_eff(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"efficientnet_b5\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.func_extract_feature import select_transformers_model, pipeline_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 20.165681838989258 ms\n",
      "outputs layers : odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "shape last_hidden_state : torch.Size([1, 197, 768])\n",
      "shape pooler_output : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_transformers_model(ViTModel, ViTImageProcessor, pretrain=\"google/vit-base-patch16-224-in21k\")\n",
    "vit_gg_pipe = pipeline_transformer(layer=\"last_hidden_state\", row=0, device=device)\n",
    "vit_gg_pipe.selct_model(model, preprocess)\n",
    "vit_gg_pipe.report_test()\n",
    "cvt_feature_vit_gg = convert_feature(vit_gg_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_vit_gg(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"vit_base_patch16_224_in21k_last_hidden_state\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 24.142026901245117 ms\n",
      "outputs layers : odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "shape last_hidden_state : torch.Size([1, 197, 768])\n",
      "shape pooler_output : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_transformers_model(ViTModel, ViTImageProcessor, \n",
    "                                              pretrain=ROOT_NFS_TEST / '/weights/vit_gg_lr2e-05_eu_9ep_0_95099acc')\n",
    "vit_gg_trained_lr2e_05_pipe = pipeline_transformer(layer=\"last_hidden_state\", row=0, device=device)\n",
    "vit_gg_trained_lr2e_05_pipe.selct_model(model, preprocess)\n",
    "vit_gg_trained_lr2e_05_pipe.report_test()\n",
    "cvt_feature_vit_gg_trained_lr2e_05 = convert_feature(vit_gg_trained_lr2e_05_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15524/15524 [20:08<00:00, 12.84it/s]\n"
     ]
    }
   ],
   "source": [
    "cvt_feature_vit_gg_trained_lr2e_05(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"vit_b_p16_224_last_hidden_trained_lr2e_05_eu_9ep_0_95099acc\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from script.func_extract_feature import select_transformers_onnx_model, pipeline_transformer_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 285.5355739593506 ms\n",
      "shape : (1, 197, 768)\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_transformers_onnx_model(\"/home/music/Desktop/measure_model/models/vit_gg/onnx/model.onnx\", \n",
    "                                                   processor=ViTImageProcessor, providers=['CPUExecutionProvider'])\n",
    "vit_gg_onnx_pipe = pipeline_transformer_onnx(layer=\"last_hidden_state\", row=0)\n",
    "vit_gg_onnx_pipe.selct_model(model, preprocess)\n",
    "vit_gg_onnx_pipe.report_test()\n",
    "cvt_feature_vit_gg_onnx = convert_feature(vit_gg_onnx_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_vit_gg_onnx(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"vit_b_p16_224_last_hidden_onnx_\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 203.07588577270508 ms\n",
      "shape : (1, 197, 768)\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_transformers_onnx_model(\"/home/music/Desktop/measure_model/models/vit_gg/onnx_quantize/model_quantized.onnx\", \n",
    "                                                   processor=ViTImageProcessor, providers=['CPUExecutionProvider'])\n",
    "vit_gg_onnx_quantize_pipe = pipeline_transformer_onnx(layer=\"last_hidden_state\", row=0)\n",
    "vit_gg_onnx_quantize_pipe.selct_model(model, preprocess)\n",
    "vit_gg_onnx_quantize_pipe.report_test()\n",
    "cvt_feature_vit_gg_onnx_quantize = convert_feature(vit_gg_onnx_quantize_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_vit_gg_onnx_quantize(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"test\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
