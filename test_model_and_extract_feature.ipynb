{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "\n",
    "from torchsummary import summary\n",
    "from timm.models import create_model\n",
    "import timm\n",
    "\n",
    "from script.tool import *\n",
    "\n",
    "from onnxruntime import InferenceSession\n",
    "from transformers import AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### image preprocessing pipeline\n",
    "IMG_SIZE = (224, 224)\n",
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "trans = [\n",
    "              transforms.Resize(IMG_SIZE),\n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
    "              ]\n",
    "\n",
    "trans = transforms.Compose(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of all image : 15524\n",
      "amount of image that less than 5 in that class : 116\n",
      "amount of image that more than 5 in that class : 15408\n"
     ]
    }
   ],
   "source": [
    "n_cv = 5\n",
    "path_dataset = '/home/music/Desktop/measure_model/data/product'\n",
    "device = torch.device(\"cuda:0\")\n",
    "df = scan_directory(path_dataset)\n",
    "df_pd, index_less_than_n, index_greater_than_or_equal_to_n = filter_data(df, minimum_data_class=n_cv)\n",
    "### load image and transfer to tensor\n",
    "img = Image.open(\"/home/music/Desktop/measure_model/data/image_net/n01514668_cock.JPEG\")\n",
    "img_tensor = trans(img).to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_model(model, img, device='cuda:0', report=True):\n",
    "    ### return specific layer\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "    start_time_torch = datetime.datetime.now()\n",
    "    output = model(transforms(img).to(device).unsqueeze(0))\n",
    "    delta_time_torch = datetime.datetime.now() - start_time_torch\n",
    "    if report:\n",
    "        print(\"model :\", model.__class__.__name__)\n",
    "        print(\"cut model\")\n",
    "        print(f\"Output shape at layer : {output.shape}\")\n",
    "        print(\"runtime :\", delta_time_torch.microseconds/1000, \"ms\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_timm(model, device='cuda:0'):\n",
    "    model.eval().to(device)\n",
    "    X_trans = []\n",
    "    first = True\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "    for img_path in tqdm(df_pd['path_img']):\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            outputs = model(transforms(img).to(device).unsqueeze(0))\n",
    "            output = outputs.flatten().unsqueeze(0)\n",
    "            output = standardize_feature(output).to('cpu').detach().numpy()\n",
    "            if first:\n",
    "                X_trans = output\n",
    "                first = False\n",
    "            else:\n",
    "                X_trans = np.concatenate((X_trans, output))\n",
    "    return X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : EfficientNet\n",
      "cut model\n",
      "Output shape at layer : torch.Size([1, 1280])\n",
      "runtime : 52.003 ms\n"
     ]
    }
   ],
   "source": [
    "efficientnet_b1 = create_model(\n",
    "        \"efficientnet_b1\",\n",
    "        num_classes=0,\n",
    "        pretrained=True,\n",
    "    )\n",
    "efficientnet_b1.eval().to(device)\n",
    "# summary(efficientformer_l3, (3, 224, 224))\n",
    "output = report_model(efficientnet_b1, img=img, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15524 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/15524 [00:00<09:57, 25.98it/s]\n"
     ]
    }
   ],
   "source": [
    "x_trans = convert_feature_timm(efficientnet_b1, device)\n",
    "save_feature(x_trans, df_pd['classes'], name=\"efficientnet_b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : EfficientNet\n",
      "cut model\n",
      "Output shape at layer : torch.Size([1, 2048])\n",
      "runtime : 542.966 ms\n"
     ]
    }
   ],
   "source": [
    "efficientnet_b5 = create_model(\n",
    "        \"efficientnet_b5\",\n",
    "        num_classes=0,\n",
    "        pretrained=True,\n",
    "    )\n",
    "efficientnet_b5.eval().to(device)\n",
    "# summary(efficientnet_b5, (3, 224, 224))\n",
    "output = report_model(efficientnet_b5, img, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## efficientformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : EfficientFormer\n",
      "cut model\n",
      "Output shape at layer : torch.Size([1, 512])\n",
      "runtime : 138.976 ms\n"
     ]
    }
   ],
   "source": [
    "efficientformer_l3 = create_model(\n",
    "        \"efficientformer_l3\",\n",
    "        num_classes=0,\n",
    "        pretrained=True,\n",
    "    )\n",
    "efficientformer_l3.eval().to('cpu')\n",
    "# summary(efficientformer_l3, (3, 224, 224))\n",
    "output = report_model(efficientformer_l3, img, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : EfficientFormer\n",
      "cut model\n",
      "Output shape at layer : torch.Size([1, 448])\n",
      "runtime : 23.688 ms\n"
     ]
    }
   ],
   "source": [
    "efficientformer_l1 = create_model(\n",
    "        \"efficientformer_l1\",\n",
    "        num_classes=0,\n",
    "        pretrained=True,\n",
    "    )\n",
    "efficientformer_l1.eval().to(device)\n",
    "# summary(efficientformer_l1, (3, 224, 224))\n",
    "output = report_model(efficientformer_l1, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : EfficientFormerV2\n",
      "cut model\n",
      "Output shape at layer : torch.Size([1, 384])\n",
      "runtime : 86.324 ms\n"
     ]
    }
   ],
   "source": [
    "efficientformerv2_l = create_model(\n",
    "        \"efficientformerv2_l\",\n",
    "        num_classes=0,\n",
    "        pretrained=True,\n",
    "    )\n",
    "efficientformerv2_l.eval().to(device)\n",
    "# summary(efficientformerv2_l, (3, 224, 224))\n",
    "output = report_model(efficientformerv2_l, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_transformer(model, processor, layer, row=False, device='cuda:0'):\n",
    "    model.eval().to(device)\n",
    "    X_trans = []\n",
    "    first = True\n",
    "    for img_path in tqdm(df_pd['path_img']):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        if type(row) == bool and row==False:\n",
    "            output = outputs[layer]\n",
    "        else:\n",
    "            output = outputs[layer][:, row]\n",
    "        output = output.flatten().unsqueeze(0)\n",
    "        output = standardize_feature(output).to('cpu').detach().numpy()\n",
    "        if first:\n",
    "            X_trans = output\n",
    "            first = False\n",
    "        else:\n",
    "            X_trans = np.concatenate((X_trans, output))\n",
    "        \n",
    "    return X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 182.445 ms\n",
      "outputs layers : odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "shape last_hidden_state : torch.Size([1, 197, 768])\n",
      "shape pooler_output : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel\n",
    "vit_gg = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "processor_vit_gg = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "vit_gg.eval().to(device)\n",
    "inputs = processor_vit_gg(images=img, return_tensors=\"pt\").to(device)\n",
    "start_time_torch = datetime.datetime.now()\n",
    "outputs = vit_gg(**inputs)\n",
    "delta_time_torch = datetime.datetime.now() - start_time_torch\n",
    "print(\"runtime :\", delta_time_torch.microseconds/1000, \"ms\")\n",
    "print(f\"outputs layers : {outputs.keys()}\")\n",
    "print(f\"shape last_hidden_state : {outputs.last_hidden_state.shape}\")\n",
    "print(f\"shape pooler_output : {outputs.pooler_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans = convert_feature_transformer(vit_gg, processor_vit_gg, layer=\"last_hidden_state\", row=0, device=device)\n",
    "save_feature(x_trans, df_pd['classes'], name=\"vit_base_patch16_224_in21k_last_hidden_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_transformer_onnx(model, processor, layer, row=False, device='cuda:0'):\n",
    "    X_trans = []\n",
    "    first = True\n",
    "    for img_path in tqdm(df_pd['path_img']):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        inputs = processor(images=img, return_tensors=\"np\")\n",
    "        outputs = model.run(output_names=[layer], input_feed=dict(inputs))[0]\n",
    "        if type(row) == bool and row==False:\n",
    "            output = outputs[0]\n",
    "        else:\n",
    "            output = outputs[:, row]\n",
    "        output = output.flatten().reshape(1, -1)\n",
    "        output = standardize_feature(output)\n",
    "        if first:\n",
    "            X_trans = output\n",
    "            first = False\n",
    "        else:\n",
    "            X_trans = np.concatenate((X_trans, output))\n",
    "        \n",
    "    return X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_gg_onnx_path = \"/home/music/Desktop/measure_model/models/vit_gg/model.onnx\"\n",
    "vit_gg_onnx = InferenceSession(vit_gg_onnx_path, providers=['CUDAExecutionProvider'])\n",
    "processor_vit_gg_onxx = AutoFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "inputs = processor_vit_gg_onxx(images=img.convert(\"RGB\"), return_tensors=\"np\")\n",
    "start_time_torch = datetime.datetime.now()\n",
    "outputs = vit_gg_onnx.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n",
    "delta_time_torch = datetime.datetime.now() - start_time_torch\n",
    "print(\"runtime :\", delta_time_torch.microseconds/1000, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15524/15524 [1:03:44<00:00,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "x_trans = convert_feature_transformer_onnx(vit_gg_onnx, processor_vit_gg, layer=\"last_hidden_state\", row=0, device=device)\n",
    "save_feature(x_trans, df_pd['classes'], name=\"vit_base_patch16_224_in21k_last_hidden_state_onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_gg_onnx_quantized_path = \"/home/music/Desktop/measure_model/models/vit_gg_onnx_quantize\"\n",
    "vit_gg_onnx_quantized_model_path = vit_gg_onnx_quantized_path+\"/model_quantized.onnx\"\n",
    "vit_gg_onnx_quantized = InferenceSession(vit_gg_onnx_quantized_model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "processor_vit_gg_onxx = AutoFeatureExtractor.from_pretrained(vit_gg_onnx_quantized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15524/15524 [55:38<00:00,  4.65it/s] \n"
     ]
    }
   ],
   "source": [
    "x_trans = convert_feature_transformer_onnx(vit_gg_onnx_quantized, processor_vit_gg_onxx, layer=\"last_hidden_state\", row=0, device=device)\n",
    "save_feature(x_trans, df_pd['classes'], name=\"vit_base_patch16_224_in21k_last_hidden_state_onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
