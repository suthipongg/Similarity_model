{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "from script.tool import ROOT, ROOT_NFS_TEST, ROOT_NFS_DATA, standardize_feature\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = ROOT_NFS_DATA / 'Cosmenet_products_15000/raw_data'\n",
    "device = torch.device(\"cuda:0\")\n",
    "df_pd = pd.read_csv(path_dataset / 'data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convert_feature:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        \n",
    "    def process_extract(self, img):\n",
    "        output = self.pipeline.extract(img)\n",
    "        return output\n",
    "    \n",
    "    def check_data(self, img_path, classes):\n",
    "        if len(img_path) != len(classes):\n",
    "            raise ValueError(\"img_path and classes must have the same length\")\n",
    "    \n",
    "    def to_pandas(self, output, classes, path):\n",
    "        df_x = pd.DataFrame(output)\n",
    "        df_y = pd.DataFrame([[classes, path]], columns=['classes', 'path_img'])\n",
    "        return pd.concat([df_x, df_y], axis=1)\n",
    "    \n",
    "    def save_data(self, output, classes, path, file_name_output, i):\n",
    "        if len(Path(file_name_output).parents) > 1:\n",
    "            path_output = Path(file_name_output).parent\n",
    "            file_name_output = str(Path(file_name_output).name)\n",
    "        else:\n",
    "            path_output = ROOT_NFS_TEST\n",
    "        path_output = path_output / \"feature_map\"\n",
    "        \n",
    "        if not os.path.exists(path_output):\n",
    "            os.makedirs(path_output)\n",
    "        path_output = path_output / str(file_name_output+'.csv')\n",
    "        \n",
    "        data = self.to_pandas(output, classes, path)\n",
    "        if i == 0:\n",
    "            data.to_csv(path_output, index=False)\n",
    "        else:\n",
    "            data.to_csv(path_output, mode='a', header=False, index=False)\n",
    "    \n",
    "    def __call__(self, img_path, classes, file_name_output=None, return_extract=False):\n",
    "        self.check_data(img_path, classes)\n",
    "        \n",
    "        for i, path in enumerate(tqdm(img_path)):\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            output = self.process_extract(img)\n",
    "            \n",
    "            if file_name_output:\n",
    "                self.save_data(output, classes[i], path, file_name_output, i)\n",
    "            \n",
    "            if return_extract:\n",
    "                if i == 0:\n",
    "                    X_trans = output\n",
    "                else:\n",
    "                    X_trans = np.concatenate((X_trans, output))\n",
    "                    \n",
    "        if return_extract: \n",
    "            return X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models import create_model\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_timm_model(model, num_classes=0, pretrain=True):\n",
    "    model = create_model(model, num_classes=num_classes, pretrained=pretrain)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    processor = timm.data.create_transform(**data_config, is_training=False)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for timm library\n",
    "class pipeline_timm:\n",
    "    def __init__(self, device='cuda:0'):\n",
    "        self.device = device\n",
    "    \n",
    "    def selct_model(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.model.eval().to(self.device)\n",
    "    \n",
    "    def process_model(self, img):\n",
    "        inputs = self.processor(img).to(self.device).unsqueeze(0)\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs\n",
    "        \n",
    "    def extract(self, img):\n",
    "        ### return specific layer\n",
    "        outputs = self.process_model(img)\n",
    "        outputs.flatten().unsqueeze(0)\n",
    "        outputs = standardize_feature(outputs).to('cpu').detach().numpy()\n",
    "        return outputs\n",
    "    \n",
    "    def report_test(self):\n",
    "        img = Image.new('RGB', (224, 224))\n",
    "        start_time_torch = time.time()\n",
    "        outputs = self.process_model(img)\n",
    "        delta_time_torch = time.time() - start_time_torch\n",
    "        print(\"runtime :\", delta_time_torch*1000, \"ms\")\n",
    "        print(f\"Output shape at layer : {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 26.701688766479492 ms\n",
      "Output shape at layer : torch.Size([1, 1280])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_timm_model('efficientnet_b1', num_classes=0, pretrain=True)\n",
    "eff_pipe = pipeline_timm(device=device)\n",
    "eff_pipe.selct_model(model, preprocess)\n",
    "eff_pipe.report_test()\n",
    "cvt_feature_eff = convert_feature(eff_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_eff(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"test\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 40.941715240478516 ms\n",
      "Output shape at layer : torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_timm_model('efficientnet_b5', num_classes=0, pretrain=True)\n",
    "eff_b5_pipe = pipeline_timm(device=device)\n",
    "eff_b5_pipe.selct_model(model, preprocess)\n",
    "eff_b5_pipe.report_test()\n",
    "cvt_feature_eff_b5 = convert_feature(eff_b5_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_eff(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"efficientnet_b5\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_transformers_model(model, processor, pretrain=\"google/vit-base-patch16-224-in21k\"):\n",
    "    model = model.from_pretrained(pretrain)\n",
    "    processor = processor.from_pretrained(pretrain)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for transformer library\n",
    "class pipeline_transformer:\n",
    "    def __init__(self, layer, row=False, device='cuda:0'):\n",
    "        self.device = device\n",
    "        self.layer = layer\n",
    "        self.row = row\n",
    "    \n",
    "    def selct_model(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.model.eval().to(self.device)\n",
    "    \n",
    "    def process_model(self, img):\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs\n",
    "        \n",
    "    def extract(self, img):\n",
    "        ### return specific layer\n",
    "        outputs = self.process_model(img)\n",
    "        if type(self.row) == bool and not self.row:\n",
    "            outputs = outputs[self.layer]\n",
    "        else:\n",
    "            outputs = outputs[self.layer][:, self.row]\n",
    "        outputs = outputs.flatten().unsqueeze(0)\n",
    "        outputs = standardize_feature(outputs).to('cpu').detach().numpy()\n",
    "        return outputs\n",
    "    \n",
    "    def report_test(self):\n",
    "        img = Image.new('RGB', (224, 224))\n",
    "        start_time_torch = time.time()\n",
    "        outputs = self.process_model(img)\n",
    "        delta_time_torch = time.time() - start_time_torch\n",
    "        print(\"runtime :\", delta_time_torch*1000, \"ms\")\n",
    "        print(f\"outputs layers : {outputs.keys()}\")\n",
    "        print(f\"shape last_hidden_state : {outputs.last_hidden_state.shape}\")\n",
    "        print(f\"shape pooler_output : {outputs.pooler_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 20.165681838989258 ms\n",
      "outputs layers : odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "shape last_hidden_state : torch.Size([1, 197, 768])\n",
      "shape pooler_output : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_transformers_model(ViTModel, ViTImageProcessor, pretrain=\"google/vit-base-patch16-224-in21k\")\n",
    "vit_gg_pipe = pipeline_transformer(layer=\"last_hidden_state\", row=0, device=device)\n",
    "vit_gg_pipe.selct_model(model, preprocess)\n",
    "vit_gg_pipe.report_test()\n",
    "cvt_feature_vit_gg = convert_feature(vit_gg_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_vit_gg(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"vit_base_patch16_224_in21k_last_hidden_state\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 24.142026901245117 ms\n",
      "outputs layers : odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "shape last_hidden_state : torch.Size([1, 197, 768])\n",
      "shape pooler_output : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_transformers_model(ViTModel, ViTImageProcessor, \n",
    "                                              pretrain=ROOT_NFS_TEST / '/weights/vit_gg_lr2e-05_eu_9ep_0_95099acc')\n",
    "vit_gg_trained_lr2e_05_pipe = pipeline_transformer(layer=\"last_hidden_state\", row=0, device=device)\n",
    "vit_gg_trained_lr2e_05_pipe.selct_model(model, preprocess)\n",
    "vit_gg_trained_lr2e_05_pipe.report_test()\n",
    "cvt_feature_vit_gg_trained_lr2e_05 = convert_feature(vit_gg_trained_lr2e_05_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15524/15524 [20:08<00:00, 12.84it/s]\n"
     ]
    }
   ],
   "source": [
    "cvt_feature_vit_gg_trained_lr2e_05(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"vit_b_p16_224_last_hidden_trained_lr2e_05_eu_9ep_0_95099acc\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from transformers import ViTImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_transformers_onnx_model(path=\"google/vit-base-patch16-224-in21k\", processor=None, providers=['CPUExecutionProvider']):\n",
    "    model = InferenceSession(path, providers=providers)\n",
    "    processor = processor.from_pretrained(Path(path).parent)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for transformer onnx library\n",
    "class pipeline_transformer_onnx:\n",
    "    def __init__(self, layer, row=False):\n",
    "        self.layer = layer\n",
    "        self.row = row\n",
    "    \n",
    "    def selct_model(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "    \n",
    "    def process_model(self, img):\n",
    "        inputs = self.processor(images=img, return_tensors=\"np\")\n",
    "        outputs = self.model.run(output_names=[self.layer], input_feed=dict(inputs))[0]\n",
    "        return outputs\n",
    "        \n",
    "    def extract(self, img):\n",
    "        ### return specific layer\n",
    "        outputs = self.process_model(img)\n",
    "        if type(self.row) == bool and not self.row:\n",
    "            outputs = outputs[0]\n",
    "        else:\n",
    "            outputs = outputs[:, self.row]\n",
    "        outputs = standardize_feature(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def report_test(self):\n",
    "        img = Image.new('RGB', (224, 224))\n",
    "        start_time_torch = time.time()\n",
    "        outputs = self.process_model(img)\n",
    "        delta_time_torch = time.time() - start_time_torch\n",
    "        print(\"runtime :\", delta_time_torch*1000, \"ms\")\n",
    "        print(f\"shape : {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 285.5355739593506 ms\n",
      "shape : (1, 197, 768)\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_transformers_onnx_model(\"/home/music/Desktop/measure_model/models/vit_gg/onnx/model.onnx\", \n",
    "                                                   processor=ViTImageProcessor, providers=['CPUExecutionProvider'])\n",
    "vit_gg_onnx_pipe = pipeline_transformer_onnx(layer=\"last_hidden_state\", row=0)\n",
    "vit_gg_onnx_pipe.selct_model(model, preprocess)\n",
    "vit_gg_onnx_pipe.report_test()\n",
    "cvt_feature_vit_gg_onnx = convert_feature(vit_gg_onnx_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_vit_gg_onnx(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"vit_b_p16_224_last_hidden_onnx_\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 203.07588577270508 ms\n",
      "shape : (1, 197, 768)\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = select_transformers_onnx_model(\"/home/music/Desktop/measure_model/models/vit_gg/onnx_quantize/model_quantized.onnx\", \n",
    "                                                   processor=ViTImageProcessor, providers=['CPUExecutionProvider'])\n",
    "vit_gg_onnx_quantize_pipe = pipeline_transformer_onnx(layer=\"last_hidden_state\", row=0)\n",
    "vit_gg_onnx_quantize_pipe.selct_model(model, preprocess)\n",
    "vit_gg_onnx_quantize_pipe.report_test()\n",
    "cvt_feature_vit_gg_onnx_quantize = convert_feature(vit_gg_onnx_quantize_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt_feature_vit_gg_onnx_quantize(\n",
    "    df_pd['path_img'], \n",
    "    df_pd['classes'], \n",
    "    file_name_output=\"test\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
