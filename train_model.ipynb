{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.func_split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of all data : 94493\n",
      "amount of all class : 21250\n",
      "amount of data 2-8 img : 3148\n",
      "amount of 2-8 img class : 571\n",
      "amount of data more 8 img : 75208\n",
      "amount of more 8 img class : 4542\n",
      "amount of data & class only one : 16137\n",
      "\n",
      "amount of train split : 49340\n",
      "amount of train split class : 3724\n",
      "amount of test split : 12338\n",
      "amount of test split class : 3724\n",
      "amount of train val : 10826\n",
      "amount of train val class : 818\n",
      "amount of test val : 2704\n",
      "amount of test val class : 818\n",
      "amount of train val mix : 12400\n",
      "amount of train val mix class : 1389\n",
      "amount of test val mix : 4278\n",
      "amount of test val mix class : 1389\n"
     ]
    }
   ],
   "source": [
    "split_df = split_data(data_path='Cosmenet_uat_20231108', data_csv='data_last_join_2023_11_10.csv')\n",
    "split_df.split_data()\n",
    "split_df.report_train_test_split()\n",
    "print()\n",
    "split_df.report_train_test_val_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_split, df_test_split = split_df.get_train_test()\n",
    "df_train_val_mix, df_test_val_mix = split_df.get_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>images_path</th>\n",
       "      <th>BID</th>\n",
       "      <th>SCID</th>\n",
       "      <th>CID</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46985_13.png</td>\n",
       "      <td>46985</td>\n",
       "      <td>/app/nfs_clientshare/Datasets/Cosmenet_product...</td>\n",
       "      <td>4169</td>\n",
       "      <td>84</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_names  labels                                        images_path  \\\n",
       "0  46985_13.png   46985  /app/nfs_clientshare/Datasets/Cosmenet_product...   \n",
       "\n",
       "    BID  SCID  CID  Action  \n",
       "0  4169    84   53       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_split.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from script.tool import ROOT_NFS, ROOT_NFS_DATA, ROOT_NFS_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = get_default_device()\n",
    "LEARNING_RATE = 0.00002\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosmenetDataset_Triplet():\n",
    "    def __init__(self, df: pd, train=True, transform=None):\n",
    "        self.data_csv = df\n",
    "        self.is_train = train\n",
    "        self.transform = transform\n",
    "        if self.is_train:\n",
    "            self.labels = df['labels'].values\n",
    "            self.image_path = df['images_path'].values\n",
    "            self.index = df.index.values \n",
    "    \n",
    "    def get_caompare_img(self, item, anchor_label, compare_type):\n",
    "        if compare_type == \"pos\":\n",
    "            compare_list = self.index[self.index!=item][self.labels[self.index!=item]==anchor_label]\n",
    "        elif compare_type == \"neg\":\n",
    "            compare_list = self.index[self.index!=item][self.labels[self.index!=item]!=anchor_label]\n",
    "        else:\n",
    "            raise ValueError(\"compare_type must be pos or neg\")\n",
    "        compare_item = random.choice(compare_list)\n",
    "        compare_image_path = self.image_path[compare_item]\n",
    "        compare_img = Image.open(compare_image_path).convert('RGB')\n",
    "        return compare_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        anchor_label = self.labels[item]\n",
    "        anchor_image_path = self.image_path[item]\n",
    "        anchor_img = Image.open(anchor_image_path).convert('RGB')\n",
    "        if self.is_train:\n",
    "            positive_img = self.get_caompare_img(item, anchor_label, \"pos\")\n",
    "            negative_img = self.get_caompare_img(item, anchor_label, \"neg\")\n",
    "            if self.transform!=None:\n",
    "                anchor_img = (self.transform(anchor_img)*255).int()\n",
    "                positive_img = (self.transform(positive_img)*255).int()\n",
    "                negative_img = (self.transform(negative_img)*255).int()\n",
    "        return anchor_img, positive_img, negative_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(IMAGE_SIZE, data):\n",
    "    trans = transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE), antialias=False)])\n",
    "    dataset = CosmenetDataset_Triplet(data, train=True, transform=trans)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(IMAGE_SIZE, df_train_split)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 09:22:01.552195: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-13 09:22:02.142755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-13 09:22:02.142812: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-13 09:22:02.145811: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-13 09:22:02.458360: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 09:22:05.946476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "\n",
    "from script.func_extract_feature import pipeline_transformer, convert_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_transformers_model(model, processor, pretrain=\"google/vit-base-patch16-224-in21k\", load_state_dict=None):\n",
    "    model = model.from_pretrained(pretrain)\n",
    "    processor = processor.from_pretrained(pretrain)\n",
    "    if load_state_dict:\n",
    "        model.load_state_dict(torch.load(load_state_dict)['model_state_dict'])\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_state_dict='./weights/temp_epoch/vitgg_lr2e05_ep3_loss0.0.pth'\n",
    "model_pipeline, preprocess = select_transformers_model(\n",
    "    ViTModel, ViTImageProcessor, pretrain=\"google/vit-base-patch16-224-in21k\", \n",
    "    # load_state_dict=load_state_dict\n",
    ")\n",
    "vit_gg_pipe = pipeline_transformer(layer=\"last_hidden_state\", row=0, device=DEVICE)\n",
    "vit_gg_pipe.selct_model(model_pipeline, preprocess)\n",
    "Optimizer = torch.optim.Adam(vit_gg_pipe.model.parameters(),lr = LEARNING_RATE)\n",
    "# Optimizer.load_state_dict(torch.load(load_state_dict)['optimizer_state_dict'])\n",
    "cvt_feature_vit_gg = convert_feature(vit_gg_pipe)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze at layer : pooler.dense.weight\n",
      "Freeze at layer : pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "# LAST_LAYER = 199\n",
    "LAST_LAYER = 198\n",
    "for n, (layer, param) in enumerate(vit_gg_pipe.model.named_parameters()):\n",
    "    if n >= LAST_LAYER:\n",
    "        print(\"Freeze at layer :\", layer)\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weight(epoch, model, Optimizer, running_loss):\n",
    "    path_trained = get_name(epoch, running_loss)\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.pipeline.model.state_dict(),\n",
    "            'optimizer_state_dict': Optimizer.state_dict(),\n",
    "            'loss': np.mean(running_loss),\n",
    "            }, path_trained)\n",
    "\n",
    "def get_name(epoch, running_loss):\n",
    "    return f\"weights/temp_epoch/vitgg_lr2e05_ep{str(epoch+1)}_loss{str(round(np.mean(running_loss), 5))}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae78606217f4e3097bfc32fb7ca1b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 — Loss: 0.0076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/40 — Loss: 0.0042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/40 — Loss: 0.0060\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/40 — Loss: 0.0055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/40 — Loss: 0.0050\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/40 — Loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/40 — Loss: 0.0032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/40 — Loss: 0.0032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/40 — Loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/40 — Loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/40 — Loss: 0.0027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/40 — Loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/40 — Loss: 0.0016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 — Loss: 0.0034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/40 — Loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f6b151e7654203851136c28b0b3531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/nfs_clientshare/mew/project_venv/similarity/lib/python3.11/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# triple loss\n",
    "for epoch in tqdm(range(3, EPOCHS), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, positive_img, negative_img) in enumerate(tqdm(train_dl, desc=\"Training\", leave=False)):\n",
    "        anchor_out = cvt_feature_vit_gg.process_extract(anchor_img, output_type='pt')\n",
    "        positive_out = cvt_feature_vit_gg.process_extract(positive_img, output_type='pt')\n",
    "        negative_out = cvt_feature_vit_gg.process_extract(negative_img, output_type='pt')\n",
    "        \n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        \n",
    "        Optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    save_weight(epoch, cvt_feature_vit_gg, Optimizer, running_loss)\n",
    "    print(\"Epoch: {}/{} — Loss: {:.4f}\".format(epoch+1, EPOCHS, np.mean(running_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/music/Desktop/measure_model/weights/vit_gg_lr2e-05_eu_40ep/preprocessor_config.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_trained = \"/home/music/Desktop/measure_model/weights/vit_gg_lr2e-05_eu_40ep\"\n",
    "model_pipeline.save_pretrained(path_trained, from_pt=True)\n",
    "preprocess.save_pretrained(path_trained, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "import torch\n",
    "path_trained = ROOT_NFS_TEST / \"weights/vit_gg_lr2e-05_eu_9ep_0_95099acc\"\n",
    "vit_gg = ViTModel.from_pretrained(path_trained)\n",
    "torch.save(vit_gg.state_dict(), './weights/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "test.load_state_dict(torch.load('./weights/model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
