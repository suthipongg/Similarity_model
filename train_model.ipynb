{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random\n",
    "from script.tool import ROOT_NFS, ROOT_NFS_DATA, ROOT_NFS_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = get_default_device()\n",
    "LEARNING_RATE = 0.00002\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosmenetDataset_Triplet():\n",
    "    def __init__(self, df: pd, path: Path, train=True, transform=None):\n",
    "        self.data_csv = df\n",
    "        self.is_train = train\n",
    "        self.transform = transform\n",
    "        self.path = path\n",
    "        if self.is_train:\n",
    "            self.images = df.iloc[:, 0].values\n",
    "            self.labels = df.iloc[:, 1].values\n",
    "            self.index = df.index.values \n",
    "    \n",
    "    def full_path(self, label,  image_name):\n",
    "        return self.path / str(label) / image_name\n",
    "    \n",
    "    def get_caompare_img(self, item, anchor_label, compare_type):\n",
    "        if compare_type == \"pos\":\n",
    "            compare_list = self.index[self.index!=item][self.labels[self.index!=item]==anchor_label]\n",
    "        elif compare_type == \"neg\":\n",
    "            compare_list = self.index[self.index!=item][self.labels[self.index!=item]!=anchor_label]\n",
    "        else:\n",
    "            raise ValueError(\"compare_type must be pos or neg\")\n",
    "        compare_item = random.choice(compare_list)\n",
    "        compare_image_name = self.images[compare_item]\n",
    "        compare_image_path = self.full_path(self.labels[compare_item], compare_image_name)\n",
    "        compare_img = Image.open(compare_image_path).convert('RGB')\n",
    "        return compare_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        anchor_label = self.labels[item]\n",
    "        anchor_image_name = self.images[item]\n",
    "        anchor_image_path = self.full_path(self.labels[item], anchor_image_name)\n",
    "        anchor_img = Image.open(anchor_image_path).convert('RGB')\n",
    "        if self.is_train:\n",
    "            positive_img = self.get_caompare_img(item, anchor_label, \"pos\")\n",
    "            negative_img = self.get_caompare_img(item, anchor_label, \"neg\")\n",
    "            if self.transform!=None:\n",
    "                anchor_img = (self.transform(anchor_img)*255).int()\n",
    "                positive_img = (self.transform(positive_img)*255).int()\n",
    "                negative_img = (self.transform(negative_img)*255).int()\n",
    "        return anchor_img, positive_img, negative_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real data\n",
    "train_data_path = ROOT_NFS_DATA / \"Cosmenet_product_20231018/datas\"\n",
    "train_data = pd.read_csv(ROOT_NFS_DATA / 'Cosmenet_product_20231018/datas_20231018.csv')\n",
    "df_group = train_data.groupby('labels',sort=False).count()\n",
    "filter_count = df_group[(df_group.file_names >= 20).values & (df_group.file_names < 40).values].index\n",
    "train_data = train_data[train_data[\"labels\"].isin(filter_count)].reset_index(drop=True)\n",
    "\n",
    "def get_train_dataset(IMAGE_SIZE):\n",
    "    trans = transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE), antialias=False)])\n",
    "    train_dataset = CosmenetDataset_Triplet(train_data, path=train_data_path, train=True, transform=trans)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_train_dataset(IMAGE_SIZE)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from script.tool import convert_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_transformers_model(model, processor, pretrain=\"google/vit-base-patch16-224-in21k\"):\n",
    "    model = model.from_pretrained(pretrain)\n",
    "    processor = processor.from_pretrained(pretrain)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for transformer library\n",
    "class pipeline_transformer:\n",
    "    def __init__(self, layer, row=False, device='cuda:0'):\n",
    "        self.device = device\n",
    "        self.layer = layer\n",
    "        self.row = row\n",
    "    \n",
    "    def selct_model(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.model.eval().to(self.device)\n",
    "    \n",
    "    def process_model(self, img):\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs\n",
    "        \n",
    "    def extract(self, img):\n",
    "        ### return specific layer\n",
    "        outputs = self.process_model(img)\n",
    "        if type(self.row) == bool and not self.row:\n",
    "            outputs = outputs[self.layer]\n",
    "        else:\n",
    "            outputs = outputs[self.layer][:, self.row]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = select_transformers_model(ViTModel, ViTImageProcessor, pretrain=\"google/vit-base-patch16-224-in21k\")\n",
    "vit_gg_pipe = pipeline_transformer(layer=\"last_hidden_state\", row=0, device=DEVICE)\n",
    "vit_gg_pipe.selct_model(model, preprocess)\n",
    "Optimizer = torch.optim.Adam(vit_gg_pipe.model.parameters(),lr = LEARNING_RATE)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-8)\n",
    "cvt_feature_vit_gg = convert_feature(vit_gg_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze at layer : pooler.dense.weight\n",
      "Freeze at layer : pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "# LAST_LAYER = 199\n",
    "LAST_LAYER = 198\n",
    "for n, (layer, param) in enumerate(vit_gg_pipe.model.named_parameters()):\n",
    "    if n >= LAST_LAYER:\n",
    "        print(\"Freeze at layer :\", layer)\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.tool import split_StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of all image : 15524\n",
      "amount of image that less than 5 in that class : 116\n",
      "amount of image that more than 5 in that class : 15408\n"
     ]
    }
   ],
   "source": [
    "n_cv = 5\n",
    "path_dataset = ROOT_NFS_DATA / \"Cosmenet_products_15000/raw_data\"\n",
    "device = torch.device(DEVICE)\n",
    "df = scan_directory(path_dataset)\n",
    "df_pd, index_less_than_n, index_greater_than_or_equal_to_n = filter_data(df, minimum_data_class=n_cv)\n",
    "y_label = df_pd['classes_labeled'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_transformer(model, processor, layer, row=False, device='cuda:0'):\n",
    "    model.eval().to(device)\n",
    "    X_trans = []\n",
    "    first = True\n",
    "    for img_path in tqdm(df_pd['path_img'], desc=\"Extract\"):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        if type(row) == bool and row==False:\n",
    "            output = outputs[layer]\n",
    "        else:\n",
    "            output = outputs[layer][:, row]\n",
    "        output = output.flatten().unsqueeze(0)\n",
    "        output = standardize_feature(output).to('cpu').detach().numpy()\n",
    "        if first:\n",
    "            X_trans = output\n",
    "            first = False\n",
    "        else:\n",
    "            X_trans = np.concatenate((X_trans, output))\n",
    "    \n",
    "    df_x = pd.DataFrame(X_trans)\n",
    "    df_y = pd.DataFrame(df_pd['classes'], columns=['classes'])\n",
    "    data = pd.concat([df_x, df_y], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data, n_cv=5):\n",
    "    x_gg, y_gg = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "    X = x_gg\n",
    "    y = y_label\n",
    "    y_gg_un = y_gg\n",
    "    index_filter=(index_greater_than_or_equal_to_n, index_less_than_n)\n",
    "\n",
    "    first = True\n",
    "    result_in_n = []\n",
    "    skf = StratifiedKFold(n_splits=n_cv)\n",
    "    if index_filter != False:\n",
    "        index_greater_filtered, index_less_filtered = index_filter\n",
    "        X_less = X[index_less_filtered]\n",
    "        y_less = y[index_less_filtered]\n",
    "        y_gg_un_less = y_gg_un[index_less_filtered]\n",
    "        X = X[index_greater_filtered]\n",
    "        y = y[index_greater_filtered]\n",
    "        y_gg_un = y_gg_un[index_greater_filtered]\n",
    "                \n",
    "    index_df_split = skf.split(X, y)\n",
    "\n",
    "    for train_index, test_index in tqdm(index_df_split,  desc=\"Validate\"):\n",
    "        x_train = np.array(X)[train_index]\n",
    "        y_train = np.array(y)[train_index]\n",
    "        y_gg_un_train = np.array(y_gg_un)[train_index]\n",
    "        x_test = np.array(X)[test_index]\n",
    "        y_test = np.array(y)[test_index]\n",
    "        y_gg_un_test = np.array(y_gg_un)[test_index]\n",
    "        \n",
    "        if index_filter != False:\n",
    "            x_train = np.concatenate((x_train, X_less))\n",
    "            y_train = np.concatenate((y_train, y_less))\n",
    "            y_gg_train = np.concatenate((y_gg_un_train, y_gg_un_less))\n",
    "\n",
    "        dot_product = np.dot(x_test,x_train.T)              # (x_test , x_train)\n",
    "        norm_test = norm(x_test, axis=1).reshape(-1, 1)     # (x_test, 1)\n",
    "        norm_train = norm(x_train, axis=1).reshape(1, -1)   # (1, x_train)\n",
    "        res = dot_product/(norm_test*norm_train)            # res = (x_test , x_train), norm_test*norm_train = (x_test , x_train)\n",
    "        \n",
    "        f = True\n",
    "        rank_top_n = []\n",
    "        ranking = np.argsort(res, axis=1)\n",
    "        y_ranking = np.repeat(y_gg_train.reshape(1, -1), repeats=ranking.shape[0], axis=0)\n",
    "        result_ranking = np.take_along_axis(y_ranking, ranking, axis=1)[:, ::-1]\n",
    "        for row in result_ranking:\n",
    "            indexes = np.unique(row, return_index=True)\n",
    "            res_row = row[sorted(indexes[1])][:5].reshape(1, -1)\n",
    "            if f:\n",
    "                f = False\n",
    "                rank_top_n = res_row\n",
    "            else:\n",
    "                rank_top_n = np.concatenate((rank_top_n, res_row))\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "            result_in_n = [rank_top_n]\n",
    "        else:\n",
    "            result_in_n.append(rank_top_n)\n",
    "        \n",
    "    result_avg = sum((y_gg_un_test.reshape(-1, 1) == result_in_n[-1]).any(axis=1))/result_in_n[-1].shape[0]\n",
    "    return result_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weight(epoch, model, Optimizer, running_loss, acc_top, acc_top_n):\n",
    "    path_trained = get_name(epoch, running_loss, acc_top, acc_top_n)\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.pipeline.model.state_dict(),\n",
    "            'optimizer_state_dict': Optimizer.state_dict(),\n",
    "            'loss': np.mean(running_loss),\n",
    "            'acc_top' : acc_top,\n",
    "            'acc_top_n' : acc_top_n\n",
    "            }, path_trained)\n",
    "\n",
    "def get_name(epoch, running_loss, acc_top, acc_top_n):\n",
    "    return f\"weights/vit_gg_lr2e_05_{str(epoch+1)}ep_{str(round(np.mean(running_loss), 5))}loss_{acc_top}acc_top_{acc_top_n}acc_top_n.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# triple loss\n",
    "best_validation_loss = 100\n",
    "best_acc_top = 0\n",
    "best_acc_top_n = 0\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, positive_img, negative_img) in enumerate(tqdm(train_dl, desc=\"Training\", leave=False)):\n",
    "        anchor_out = cvt_feature_vit_gg.process_extract(anchor_img)\n",
    "        positive_out = cvt_feature_vit_gg.process_extract(positive_img)\n",
    "        negative_out = cvt_feature_vit_gg.process_extract(negative_img)\n",
    "        \n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        \n",
    "        Optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    x_trans = convert_feature_transformer(vit_gg, processor_vit_gg, layer=\"last_hidden_state\", row=0, device=DEVICE)\n",
    "    validation_loss = validate(x_trans)\n",
    "    acc_top = \n",
    "    acc_top_n = \n",
    "    \n",
    "    if acc_top > best_acc_top:\n",
    "        best_acc_top = acc_top\n",
    "        save_weight(epoch, cvt_feature_vit_gg, Optimizer, running_loss, acc_top, acc_top_n)\n",
    "        print(\"Save model at Best acc top: {:.4f}\".format(best_acc_top))\n",
    "        \n",
    "    if acc_top_n > best_acc_top_n:\n",
    "        best_acc_top_n = acc_top_n\n",
    "        save_weight(epoch, cvt_feature_vit_gg, Optimizer, running_loss, acc_top, acc_top_n)\n",
    "        print(\"Save model at Best acc top n: {:.4f}\".format(best_acc_top_n))\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss  # Update the best performance\n",
    "        save_weight(epoch, cvt_feature_vit_gg, Optimizer, running_loss, acc_top, acc_top_n)\n",
    "        print(\"Save model at Best performance: {:.4f}\".format(best_validation_loss))\n",
    "    \n",
    "    if epoch%10 == 9:\n",
    "        save_weight(epoch, cvt_feature_vit_gg, Optimizer, running_loss, acc_top, acc_top_n)\n",
    "        print(\"Save model at epoch {}\".format(epoch+1))\n",
    "\n",
    "    print(\"Epoch: {}/{} — Loss: {:.4f} — validation_loss : {:.4f} — acc_top : {} — acc_top_n : {}\".format(\n",
    "        epoch+1, EPOCHS, np.mean(running_loss), validation_loss, acc_top, acc_top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/music/Desktop/measure_model/weights/vit_gg_lr2e-05_eu_40ep/preprocessor_config.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_trained = \"/home/music/Desktop/measure_model/weights/vit_gg_lr2e-05_eu_40ep\"\n",
    "vit_gg.save_pretrained(path_trained, from_pt=True)\n",
    "processor_vit_gg.save_pretrained(path_trained, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "import torch\n",
    "path_trained = ROOT_NFS_TEST / \"weights/vit_gg_lr2e-05_eu_9ep_0_95099acc\"\n",
    "vit_gg = ViTModel.from_pretrained(path_trained)\n",
    "torch.save(vit_gg.state_dict(), './weights/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "test.load_state_dict(torch.load('./weights/model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
