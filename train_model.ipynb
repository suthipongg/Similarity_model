{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create train_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cosmenet = Path(\"/home/music/Desktop/measure_model/data/cosmenet_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for root, directories, files in os.walk(path_cosmenet):\n",
    "    for file in files:\n",
    "        classes = Path(root).name\n",
    "        df.append([file, classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = pd.DataFrame(df, columns=['image_name', 'id_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.to_csv(path_cosmenet / 'train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosmenetDataset_Triplet():\n",
    "    def __init__(self, df: pd, path: Path, train=True, transform=None):\n",
    "        self.data_csv = df\n",
    "        self.is_train = train\n",
    "        self.transform = transform\n",
    "        self.path = path\n",
    "        if self.is_train:\n",
    "            self.images = df.iloc[:, 0].values\n",
    "            self.labels = df.iloc[:, 1].values\n",
    "            self.index = df.index.values \n",
    "    \n",
    "    def full_path(self, label,  image_name):\n",
    "        return self.path / str(label) / image_name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        anchor_image_name = self.images[item]\n",
    "        anchor_image_path = self.full_path(self.labels[item], anchor_image_name)\n",
    "        ###### Anchor Image #######\n",
    "        anchor_img = Image.open(anchor_image_path).convert('RGB')\n",
    "        if self.is_train:\n",
    "            anchor_label = self.labels[item]\n",
    "            positive_list = self.index[self.index!=item][self.labels[self.index!=item]==anchor_label]\n",
    "            positive_item = random.choice(positive_list)\n",
    "            positive_image_name = self.images[positive_item]\n",
    "            positive_image_path = self.full_path(self.labels[positive_item], positive_image_name)\n",
    "            positive_img = Image.open(positive_image_path).convert('RGB')\n",
    "            #positive_img = self.images[positive_item].reshape(28, 28, 1)\n",
    "            negative_list = self.index[self.index!=item][self.labels[self.index!=item]!=anchor_label]\n",
    "            negative_item = random.choice(negative_list)\n",
    "            negative_image_name = self.images[negative_item]\n",
    "            negative_image_path = self.full_path(self.labels[negative_item], negative_image_name)\n",
    "            negative_img = Image.open(negative_image_path).convert('RGB')\n",
    "            #negative_img = self.images[negative_item].reshape(28, 28, 1)\n",
    "            if self.transform!=None:\n",
    "                anchor_img = self.transform(anchor_img)\n",
    "                positive_img = self.transform(positive_img)                   \n",
    "                negative_img = self.transform(negative_img)\n",
    "        return anchor_img, positive_img, negative_img, anchor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = Path(\"/home/music/Desktop/measure_model/data/cosmenet_test\")\n",
    "train_data= pd.read_csv(train_data_path / 'train_data.csv') # [imag_path, label]\n",
    "def get_train_dataset(IMAGE_SIZE):\n",
    "    trans = transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE), antialias=False)])\n",
    "    train_dataset = CosmenetDataset_Triplet(train_data, path=train_data_path, train=True, transform=trans)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "#””Pick GPU if available, else CPU”””\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 1\n",
    "DEVICE = get_default_device()\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_train_dataset(IMAGE_SIZE)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    def calc_euclidean(self, x1, x2):\n",
    "        return (x1 - x2).pow(2).sum(1)\n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
    "        distance_positive = self.calc_euclidean(anchor, positive)\n",
    "        distance_negative = self.calc_euclidean(anchor, negative)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel\n",
    "vit_gg = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "vit_gg.eval().to(DEVICE)\n",
    "processor_vit_gg = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "Optimizer = torch.optim.Adam(vit_gg.parameters(),lr = LEARNING_RATE)\n",
    "criterion = TripletLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST_LAYER = 199\n",
    "LAST_LAYER = -1\n",
    "for n, layer in enumerate(vit_gg.parameters()):\n",
    "    if n >= LAST_LAYER:\n",
    "        layer.requires_grad = False\n",
    "    else:\n",
    "        layer.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "for epoch in tqdm(range(1), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, positive_img, negative_img, anchor_label) in enumerate(tqdm(train_dl, desc=\"Training\", leave=False)):\n",
    "        anchor_img = processor_vit_gg(images=anchor_img, return_tensors=\"pt\").to(DEVICE)\n",
    "        positive_img = processor_vit_gg(images=positive_img, return_tensors=\"pt\").to(DEVICE)\n",
    "        negative_img = processor_vit_gg(images=negative_img, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        anchor_out = vit_gg(**anchor_img)\n",
    "        positive_out = vit_gg(**positive_img)\n",
    "        negative_out = vit_gg(**negative_img)\n",
    "        \n",
    "        # loss = criterion(anchor_out, positive_out, negative_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]/home/music/Desktop/scanner_venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/music/Desktop/scanner_venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/music/Desktop/scanner_venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, positive_img, negative_img, anchor_label) in enumerate(tqdm(train_dl, desc=\"Training\", leave=False)):\n",
    "        anchor_img = processor_vit_gg(images=anchor_img, return_tensors=\"pt\").to(DEVICE)\n",
    "        positive_img = processor_vit_gg(images=positive_img, return_tensors=\"pt\").to(DEVICE)\n",
    "        negative_img = processor_vit_gg(images=negative_img, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        anchor_out = vit_gg(**anchor_img)\n",
    "        positive_out = vit_gg(**positive_img)\n",
    "        negative_out = vit_gg(**negative_img)\n",
    "        \n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        \n",
    "        Optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "        print(\"Epoch: {}/{} — Loss: {:.4f}\".format(epoch+1, EPOCHS, np.mean(running_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
