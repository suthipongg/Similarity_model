{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from script.tool import ROOT_NFS_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>images_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14624_14.jpg</td>\n",
       "      <td>14624</td>\n",
       "      <td>/app/nfs_clientshare/Datasets/Cosmenet_product...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_names  labels                                        images_path\n",
       "0  14624_14.jpg   14624  /app/nfs_clientshare/Datasets/Cosmenet_product..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dataset = ROOT_NFS_DATA / 'Cosmenet_product_20231018'\n",
    "df_pd = pd.read_csv(path_dataset / 'datas_20231018.csv')\n",
    "df_pd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>50348</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels  count\n",
       "4172   50348    100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_df = df_pd.groupby(['labels'])['labels'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "group_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_img_2_to_8 = group_df[(group_df['count'] <= 8) & (group_df['count'] > 1)]['labels'].values\n",
    "filter_img_1_to_8 = group_df[group_df['count'] <= 8]['labels'].values\n",
    "\n",
    "df_more_8 = df_pd[~df_pd['labels'].isin(filter_img_1_to_8)]\n",
    "df_2_to_8 = df_pd[df_pd['labels'].isin(filter_img_2_to_8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>images_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>39856_2.png</td>\n",
       "      <td>39856</td>\n",
       "      <td>/app/nfs_clientshare/Datasets/Cosmenet_product...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_names  labels                                        images_path\n",
       "322  39856_2.png   39856  /app/nfs_clientshare/Datasets/Cosmenet_product..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "train_2_to_8, test_2_to_8 = skf.split(df_2_to_8, df_2_to_8['labels']).__next__()\n",
    "df_2_to_8_train = df_2_to_8.iloc[train_2_to_8]\n",
    "df_2_to_8_test = df_2_to_8.iloc[test_2_to_8]\n",
    "df_2_to_8_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>images_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>11596_2.jpg</td>\n",
       "      <td>11596</td>\n",
       "      <td>/app/nfs_clientshare/Datasets/Cosmenet_product...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_names  labels                                        images_path\n",
       "3177  11596_2.jpg   11596  /app/nfs_clientshare/Datasets/Cosmenet_product..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss_train = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = sss_train.split(df_more_8, df_more_8['labels']).__next__()\n",
    "df_train = df_more_8.iloc[train_idx]\n",
    "df_test = df_more_8.iloc[test_idx]\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of all data : 60196\n",
      "amount of all class : 4178\n",
      "amount of data 2-8 img : 1548\n",
      "amount of 2-8 img class : 278\n",
      "amount of data more 8 img : 58631\n",
      "amount of more 8 img class : 3883\n",
      "amount of data & class only one : 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"amount of all data : {df_pd.__len__()}\")\n",
    "print(f\"amount of all class : {group_df.__len__()}\")\n",
    "print(f\"amount of data 2-8 img : {df_2_to_8.__len__()}\")\n",
    "print(f\"amount of 2-8 img class : {filter_img_2_to_8.__len__()}\")\n",
    "print(f\"amount of data more 8 img : {df_more_8.__len__()}\")\n",
    "print(f\"amount of more 8 img class : {group_df[group_df['count'] > 8]['labels'].__len__()}\")\n",
    "print(f\"amount of data & class only one : {group_df[group_df['count'] == 1]['labels'].__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_8 = group_df[(group_df['count'] > 8) & (group_df['count'] > 1)]\n",
    "group_df_count_8 = df_count_8.groupby(['count'])['count'] \\\n",
    "    .count().reset_index(name='counter_count').sort_values(['counter_count'], ascending=False)\n",
    "counter_count_1 = group_df_count_8[group_df_count_8[\"counter_count\"] == 1][\"count\"].values\n",
    "ind_c = group_df_count_8[group_df_count_8[\"counter_count\"] == 1][\"count\"].index\n",
    "df_count_8.loc[df_count_8[\"count\"].isin(counter_count_1), \"count\"] = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46783, 44043, 40575, 44033, 48695])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.18, random_state=42)\n",
    "split_idx, val_idx = sss_val.split(df_count_8, df_count_8['count']).__next__()\n",
    "split_class = df_count_8.iloc[split_idx][\"labels\"].values\n",
    "val_class = df_count_8.iloc[val_idx][\"labels\"].values\n",
    "val_class[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_split = df_train[df_train[\"labels\"].isin(split_class)]\n",
    "df_test_split = df_test[df_test[\"labels\"].isin(split_class)]\n",
    "df_train_val = df_train[df_train[\"labels\"].isin(val_class)]\n",
    "df_test_val = df_test[df_test[\"labels\"].isin(val_class)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_mix = pd.concat([df_train_val, df_2_to_8_train])\n",
    "df_test_val_mix = pd.concat([df_test_val, df_2_to_8_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of train split : 38474\n",
      "amount of train split class : 3184\n",
      "amount of test split : 9620\n",
      "amount of test split class : 3184\n",
      "amount of train val : 8430\n",
      "amount of train val class : 699\n",
      "amount of test val : 2107\n",
      "amount of test val class : 699\n",
      "amount of train val mix : 9204\n",
      "amount of train val mix class : 977\n",
      "amount of test val mix : 2881\n",
      "amount of test val mix class : 977\n"
     ]
    }
   ],
   "source": [
    "print(f\"amount of train split : {len(df_train_split)}\")\n",
    "print(f\"amount of train split class : {df_train_split['labels'].nunique()}\")\n",
    "print(f\"amount of test split : {len(df_test_split)}\")\n",
    "print(f\"amount of test split class : {df_test_split['labels'].nunique()}\")\n",
    "print(f\"amount of train val : {len(df_train_val)}\")\n",
    "print(f\"amount of train val class : {df_train_val['labels'].nunique()}\")\n",
    "print(f\"amount of test val : {len(df_test_val)}\")\n",
    "print(f\"amount of test val class : {df_test_val['labels'].nunique()}\")\n",
    "print(f\"amount of train val mix : {len(df_train_val_mix)}\")\n",
    "print(f\"amount of train val mix class : {df_train_val_mix['labels'].nunique()}\")\n",
    "print(f\"amount of test val mix : {len(df_test_val_mix)}\")\n",
    "print(f\"amount of test val mix class : {df_test_val_mix['labels'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_split = df_train_split.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from script.tool import ROOT_NFS, ROOT_NFS_DATA, ROOT_NFS_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = get_default_device()\n",
    "LEARNING_RATE = 0.00002\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosmenetDataset_Triplet():\n",
    "    def __init__(self, df: pd, train=True, transform=None):\n",
    "        self.data_csv = df\n",
    "        self.is_train = train\n",
    "        self.transform = transform\n",
    "        if self.is_train:\n",
    "            self.labels = df.iloc[:, 1].values\n",
    "            self.image_path = df.iloc[:, 2].values\n",
    "            self.index = df.index.values \n",
    "    \n",
    "    def get_caompare_img(self, item, anchor_label, compare_type):\n",
    "        if compare_type == \"pos\":\n",
    "            compare_list = self.index[self.index!=item][self.labels[self.index!=item]==anchor_label]\n",
    "        elif compare_type == \"neg\":\n",
    "            compare_list = self.index[self.index!=item][self.labels[self.index!=item]!=anchor_label]\n",
    "        else:\n",
    "            raise ValueError(\"compare_type must be pos or neg\")\n",
    "        compare_item = random.choice(compare_list)\n",
    "        compare_image_path = self.image_path[compare_item]\n",
    "        compare_img = Image.open(compare_image_path).convert('RGB')\n",
    "        return compare_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        anchor_label = self.labels[item]\n",
    "        anchor_image_path = self.image_path[item]\n",
    "        anchor_img = Image.open(anchor_image_path).convert('RGB')\n",
    "        if self.is_train:\n",
    "            positive_img = self.get_caompare_img(item, anchor_label, \"pos\")\n",
    "            negative_img = self.get_caompare_img(item, anchor_label, \"neg\")\n",
    "            if self.transform!=None:\n",
    "                anchor_img = (self.transform(anchor_img)*255).int()\n",
    "                positive_img = (self.transform(positive_img)*255).int()\n",
    "                negative_img = (self.transform(negative_img)*255).int()\n",
    "        return anchor_img, positive_img, negative_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(IMAGE_SIZE, data):\n",
    "    trans = transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE), antialias=False)])\n",
    "    dataset = CosmenetDataset_Triplet(data, train=True, transform=trans)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>images_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11596_2.jpg</td>\n",
       "      <td>11596</td>\n",
       "      <td>/app/nfs_clientshare/Datasets/Cosmenet_product...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_names  labels                                        images_path\n",
       "0  11596_2.jpg   11596  /app/nfs_clientshare/Datasets/Cosmenet_product..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_split.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(IMAGE_SIZE, df_train_split)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 18:07:15.025999: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-29 18:07:15.546304: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-29 18:07:15.546348: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-29 18:07:15.548359: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-29 18:07:15.756451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-29 18:07:18.795753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from script.tool import convert_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_transformers_model(model, processor, pretrain=\"google/vit-base-patch16-224-in21k\", load_state_dict=None):\n",
    "    model = model.from_pretrained(pretrain)\n",
    "    processor = processor.from_pretrained(pretrain)\n",
    "    Optimizer = torch.optim.Adam(model.model.parameters(),lr = LEARNING_RATE)\n",
    "    if load_state_dict:\n",
    "        model.load_state_dict(torch.load(load_state_dict)['model_state_dict'])\n",
    "        Optimizer.load_state_dict(torch.load(load_state_dict)['optimizer_state_dict'])\n",
    "    return model, processor, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for transformer library\n",
    "class pipeline_transformer:\n",
    "    def __init__(self, layer, row=False, device='cuda:0'):\n",
    "        self.device = device\n",
    "        self.layer = layer\n",
    "        self.row = row\n",
    "    \n",
    "    def selct_model(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.model.eval().to(self.device)\n",
    "    \n",
    "    def process_model(self, img):\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs\n",
    "        \n",
    "    def extract(self, img):\n",
    "        ### return specific layer\n",
    "        outputs = self.process_model(img)\n",
    "        if type(self.row) == bool and not self.row:\n",
    "            outputs = outputs[self.layer]\n",
    "        else:\n",
    "            outputs = outputs[self.layer][:, self.row]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline, preprocess, Optimizer = select_transformers_model(\n",
    "    ViTModel, ViTImageProcessor, pretrain=\"google/vit-base-patch16-224-in21k\", \n",
    "    load_state_dict='./weights/temp_epoch/vitgg_lr2e05_ep1_loss0.04568.pt')\n",
    "vit_gg_pipe = pipeline_transformer(layer=\"last_hidden_state\", row=0, device=DEVICE)\n",
    "vit_gg_pipe.selct_model(model_pipeline, preprocess)\n",
    "cvt_feature_vit_gg = convert_feature(vit_gg_pipe)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze at layer : pooler.dense.weight\n",
      "Freeze at layer : pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "# LAST_LAYER = 199\n",
    "LAST_LAYER = 198\n",
    "for n, (layer, param) in enumerate(vit_gg_pipe.model.named_parameters()):\n",
    "    if n >= LAST_LAYER:\n",
    "        print(\"Freeze at layer :\", layer)\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weight(epoch, model, Optimizer, running_loss):\n",
    "    path_trained = get_name(epoch, running_loss)\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.pipeline.model.state_dict(),\n",
    "            'optimizer_state_dict': Optimizer.state_dict(),\n",
    "            'loss': np.mean(running_loss),\n",
    "            }, path_trained)\n",
    "\n",
    "def get_name(epoch, running_loss):\n",
    "    return f\"weights/temp_epoch/vitgg_lr2e05_ep{str(epoch+1)}_loss{str(round(np.mean(running_loss), 5))}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d05ae9118142f2a35c62b271d68fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289f908aa2684ccd9fc14da722e45ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/9619 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# triple loss\n",
    "for epoch in tqdm(range(1, EPOCHS), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, positive_img, negative_img) in enumerate(tqdm(train_dl, desc=\"Training\", leave=False)):\n",
    "        anchor_out = cvt_feature_vit_gg.process_extract(anchor_img)\n",
    "        positive_out = cvt_feature_vit_gg.process_extract(positive_img)\n",
    "        negative_out = cvt_feature_vit_gg.process_extract(negative_img)\n",
    "        \n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        \n",
    "        Optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    save_weight(epoch, cvt_feature_vit_gg, Optimizer, running_loss)\n",
    "    print(\"Epoch: {}/{} — Loss: {:.4f}\".format(epoch+1, EPOCHS, np.mean(running_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/music/Desktop/measure_model/weights/vit_gg_lr2e-05_eu_40ep/preprocessor_config.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_trained = \"/home/music/Desktop/measure_model/weights/vit_gg_lr2e-05_eu_40ep\"\n",
    "model_pipeline.save_pretrained(path_trained, from_pt=True)\n",
    "preprocess.save_pretrained(path_trained, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "import torch\n",
    "path_trained = ROOT_NFS_TEST / \"weights/vit_gg_lr2e-05_eu_9ep_0_95099acc\"\n",
    "vit_gg = ViTModel.from_pretrained(path_trained)\n",
    "torch.save(vit_gg.state_dict(), './weights/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "test.load_state_dict(torch.load('./weights/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.load('./weights/temp_epoch/vitgg_lr2e05_ep1_loss0.32437.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state', 'param_groups'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['optimizer_state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['optimizer_state_dict']['state'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['step', 'exp_avg', 'exp_avg_sq'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['optimizer_state_dict']['state'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['optimizer_state_dict']['state'][0]['exp_avg_sq'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lr': 2e-05,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0,\n",
       "  'amsgrad': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'capturable': False,\n",
       "  'differentiable': False,\n",
       "  'fused': None,\n",
       "  'params': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   164,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   198,\n",
       "   199]}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['optimizer_state_dict']['param_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embeddings.cls_token', 'embeddings.position_embeddings', 'embeddings.patch_embeddings.projection.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_after.bias', 'layernorm.weight', 'layernorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['model_state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['model_state_dict']['encoder.layer.11.layernorm_before.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
