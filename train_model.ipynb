{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from script.tool import ROOT_NFS_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = ROOT_NFS_DATA / 'Cosmenet_product_20231018'\n",
    "df_pd = pd.read_csv(path_dataset / 'datas_20231018.csv')\n",
    "df_pd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = df_pd.groupby(['labels'])['labels'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "group_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_img_2_to_8 = group_df[(group_df['count'] <= 8) & (group_df['count'] > 1)]['labels'].values\n",
    "filter_img_1_to_8 = group_df[group_df['count'] <= 8]['labels'].values\n",
    "\n",
    "df_more_8 = df_pd[~df_pd['labels'].isin(filter_img_1_to_8)]\n",
    "df_2_to_8 = df_pd[df_pd['labels'].isin(filter_img_2_to_8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "train_2_to_8, test_2_to_8 = skf.split(df_2_to_8, df_2_to_8['labels']).__next__()\n",
    "df_2_to_8_train = df_2_to_8.iloc[train_2_to_8]\n",
    "df_2_to_8_test = df_2_to_8.iloc[test_2_to_8]\n",
    "df_2_to_8_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_train = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = sss_train.split(df_more_8, df_more_8['labels']).__next__()\n",
    "df_train = df_more_8.iloc[train_idx]\n",
    "df_test = df_more_8.iloc[test_idx]\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"amount of all data : {df_pd.__len__()}\")\n",
    "print(f\"amount of all class : {group_df.__len__()}\")\n",
    "print(f\"amount of data 2-8 img : {df_2_to_8.__len__()}\")\n",
    "print(f\"amount of 2-8 img class : {filter_img_2_to_8.__len__()}\")\n",
    "print(f\"amount of data more 8 img : {df_more_8.__len__()}\")\n",
    "print(f\"amount of more 8 img class : {group_df[group_df['count'] > 8]['labels'].__len__()}\")\n",
    "print(f\"amount of data & class only one : {group_df[group_df['count'] == 1]['labels'].__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_8 = group_df[(group_df['count'] > 8) & (group_df['count'] > 1)]\n",
    "group_df_count_8 = df_count_8.groupby(['count'])['count'] \\\n",
    "    .count().reset_index(name='counter_count').sort_values(['counter_count'], ascending=False)\n",
    "counter_count_1 = group_df_count_8[group_df_count_8[\"counter_count\"] == 1][\"count\"].values\n",
    "ind_c = group_df_count_8[group_df_count_8[\"counter_count\"] == 1][\"count\"].index\n",
    "df_count_8.loc[df_count_8[\"count\"].isin(counter_count_1), \"count\"] = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.18, random_state=42)\n",
    "split_idx, val_idx = sss_val.split(df_count_8, df_count_8['count']).__next__()\n",
    "split_class = df_count_8.iloc[split_idx][\"labels\"].values\n",
    "val_class = df_count_8.iloc[val_idx][\"labels\"].values\n",
    "val_class[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_split = df_train[df_train[\"labels\"].isin(split_class)]\n",
    "df_test_split = df_test[df_test[\"labels\"].isin(split_class)]\n",
    "df_train_val = df_train[df_train[\"labels\"].isin(val_class)]\n",
    "df_test_val = df_test[df_test[\"labels\"].isin(val_class)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_mix = pd.concat([df_train_val, df_2_to_8_train])\n",
    "df_test_val_mix = pd.concat([df_test_val, df_2_to_8_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"amount of train split : {len(df_train_split)}\")\n",
    "print(f\"amount of train split class : {df_train_split['labels'].nunique()}\")\n",
    "print(f\"amount of test split : {len(df_test_split)}\")\n",
    "print(f\"amount of test split class : {df_test_split['labels'].nunique()}\")\n",
    "print(f\"amount of train val : {len(df_train_val)}\")\n",
    "print(f\"amount of train val class : {df_train_val['labels'].nunique()}\")\n",
    "print(f\"amount of test val : {len(df_test_val)}\")\n",
    "print(f\"amount of test val class : {df_test_val['labels'].nunique()}\")\n",
    "print(f\"amount of train val mix : {len(df_train_val_mix)}\")\n",
    "print(f\"amount of train val mix class : {df_train_val_mix['labels'].nunique()}\")\n",
    "print(f\"amount of test val mix : {len(df_test_val_mix)}\")\n",
    "print(f\"amount of test val mix class : {df_test_val_mix['labels'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from script.tool import ROOT_NFS, ROOT_NFS_DATA, ROOT_NFS_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = get_default_device()\n",
    "LEARNING_RATE = 0.00002\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosmenetDataset_Triplet():\n",
    "    def __init__(self, df: pd, train=True, transform=None):\n",
    "        self.data_csv = df\n",
    "        self.is_train = train\n",
    "        self.transform = transform\n",
    "        if self.is_train:\n",
    "            self.labels = df.iloc[:, 1].values\n",
    "            self.image_path = df.iloc[:, 2].values\n",
    "            self.index = df.index.values \n",
    "    \n",
    "    def get_caompare_img(self, item, anchor_label, compare_type):\n",
    "        if compare_type == \"pos\":\n",
    "            compare_list = self.index[self.index!=item][self.labels[self.index!=item]==anchor_label]\n",
    "        elif compare_type == \"neg\":\n",
    "            compare_list = self.index[self.index!=item][self.labels[self.index!=item]!=anchor_label]\n",
    "        else:\n",
    "            raise ValueError(\"compare_type must be pos or neg\")\n",
    "        compare_item = random.choice(compare_list)\n",
    "        compare_image_path = self.image_path[compare_item]\n",
    "        compare_img = Image.open(compare_image_path).convert('RGB')\n",
    "        return compare_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        anchor_label = self.labels[item]\n",
    "        anchor_image_path = self.image_path[item]\n",
    "        anchor_img = Image.open(anchor_image_path).convert('RGB')\n",
    "        if self.is_train:\n",
    "            positive_img = self.get_caompare_img(item, anchor_label, \"pos\")\n",
    "            negative_img = self.get_caompare_img(item, anchor_label, \"neg\")\n",
    "            if self.transform!=None:\n",
    "                anchor_img = (self.transform(anchor_img)*255).int()\n",
    "                positive_img = (self.transform(positive_img)*255).int()\n",
    "                negative_img = (self.transform(negative_img)*255).int()\n",
    "        return anchor_img, positive_img, negative_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(IMAGE_SIZE, data):\n",
    "    trans = transforms.Compose([transforms.ToTensor(),transforms.Resize((IMAGE_SIZE,IMAGE_SIZE), antialias=False)])\n",
    "    dataset = CosmenetDataset_Triplet(data, train=True, transform=trans)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>images_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14624_14.jpg</td>\n",
       "      <td>14624</td>\n",
       "      <td>/app/nfs_clientshare/Datasets/Cosmenet_product...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_names  labels                                        images_path\n",
       "0  14624_14.jpg   14624  /app/nfs_clientshare/Datasets/Cosmenet_product..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_split.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(IMAGE_SIZE, df_train_split)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from script.tool import convert_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_transformers_model(model, processor, pretrain=\"google/vit-base-patch16-224-in21k\"):\n",
    "    model = model.from_pretrained(pretrain)\n",
    "    processor = processor.from_pretrained(pretrain)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for transformer library\n",
    "class pipeline_transformer:\n",
    "    def __init__(self, layer, row=False, device='cuda:0'):\n",
    "        self.device = device\n",
    "        self.layer = layer\n",
    "        self.row = row\n",
    "    \n",
    "    def selct_model(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.model.eval().to(self.device)\n",
    "    \n",
    "    def process_model(self, img):\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs\n",
    "        \n",
    "    def extract(self, img):\n",
    "        ### return specific layer\n",
    "        outputs = self.process_model(img)\n",
    "        if type(self.row) == bool and not self.row:\n",
    "            outputs = outputs[self.layer]\n",
    "        else:\n",
    "            outputs = outputs[self.layer][:, self.row]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline, preprocess = select_transformers_model(ViTModel, ViTImageProcessor, pretrain=\"google/vit-base-patch16-224-in21k\")\n",
    "vit_gg_pipe = pipeline_transformer(layer=\"last_hidden_state\", row=0, device=DEVICE)\n",
    "vit_gg_pipe.selct_model(model_pipeline, preprocess)\n",
    "Optimizer = torch.optim.Adam(vit_gg_pipe.model.parameters(),lr = LEARNING_RATE)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-8)\n",
    "cvt_feature_vit_gg = convert_feature(vit_gg_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze at layer : pooler.dense.weight\n",
      "Freeze at layer : pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "# LAST_LAYER = 199\n",
    "LAST_LAYER = 198\n",
    "for n, (layer, param) in enumerate(vit_gg_pipe.model.named_parameters()):\n",
    "    if n >= LAST_LAYER:\n",
    "        print(\"Freeze at layer :\", layer)\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weight(epoch, model, Optimizer, running_loss):\n",
    "    path_trained = get_name(epoch, running_loss)\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.pipeline.model.state_dict(),\n",
    "            'optimizer_state_dict': Optimizer.state_dict(),\n",
    "            'loss': np.mean(running_loss),\n",
    "            }, path_trained)\n",
    "\n",
    "def get_name(epoch, running_loss):\n",
    "    return f\"weights/vitgg_lr2e05_ep{str(epoch+1)}_loss{str(round(np.mean(running_loss), 5))}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# triple loss\n",
    "best_validation_loss = 100\n",
    "best_acc_top = 0\n",
    "best_acc_top_n = 0\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, positive_img, negative_img) in enumerate(tqdm(train_dl, desc=\"Training\", leave=False)):\n",
    "        anchor_out = cvt_feature_vit_gg.process_extract(anchor_img)\n",
    "        positive_out = cvt_feature_vit_gg.process_extract(positive_img)\n",
    "        negative_out = cvt_feature_vit_gg.process_extract(negative_img)\n",
    "        \n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        \n",
    "        Optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    save_weight(epoch, cvt_feature_vit_gg, Optimizer, running_loss)\n",
    "    print(\"Epoch: {}/{} — Loss: {:.4f}\".format(epoch+1, EPOCHS, np.mean(running_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/music/Desktop/measure_model/weights/vit_gg_lr2e-05_eu_40ep/preprocessor_config.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_trained = \"/home/music/Desktop/measure_model/weights/vit_gg_lr2e-05_eu_40ep\"\n",
    "model_pipeline.save_pretrained(path_trained, from_pt=True)\n",
    "preprocess.save_pretrained(path_trained, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "import torch\n",
    "path_trained = ROOT_NFS_TEST / \"weights/vit_gg_lr2e-05_eu_9ep_0_95099acc\"\n",
    "vit_gg = ViTModel.from_pretrained(path_trained)\n",
    "torch.save(vit_gg.state_dict(), './weights/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "test.load_state_dict(torch.load('./weights/model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
