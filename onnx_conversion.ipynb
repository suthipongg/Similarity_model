{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from onnxruntime import InferenceSession\n",
    "from onnxruntime.quantization import QuantType\n",
    "from transformers import AutoFeatureExtractor\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vit google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "/home/music/.local/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Using the export variant default. Available variants are:\n",
      "\t- default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.0+cu121\n",
      "/home/music/.local/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:170: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "/home/music/.local/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:176: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n"
     ]
    }
   ],
   "source": [
    "vit_gg_onnx = ORTModelForFeatureExtraction.from_pretrained('/app/nfs_clientshare/mew/project/Similarity_model/weights/vitgg_lr2e05_ep3_loss0.0', export=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n"
     ]
    }
   ],
   "source": [
    "# convert to onnx\n",
    "vit_gg_onnx = ORTModelForFeatureExtraction.from_pretrained('/app/nfs_clientshare/mew/project/Similarity_model/weights/vitgg_lr2e05_ep3_loss0.0', export=True)\n",
    "processor_vit_gg_onnx = AutoFeatureExtractor.from_pretrained('/app/nfs_clientshare/mew/project/Similarity_model/weights/vitgg_lr2e05_ep3_loss0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization\n",
    "quantizer = ORTQuantizer.from_pretrained(vit_gg_onnx)\n",
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "dqconfig.weights_dtype = QuantType.QUInt8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"/home/music/Desktop/measure_model/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model onnx\n",
    "vit_gg_onnx.save_pretrained(save_directory+\"vit_gg_onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/u8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: /home/music/Desktop/measure_model/models/vit_gg_onnx_quantize (external data format: False)\n",
      "Configuration saved in /home/music/Desktop/measure_model/models/vit_gg_onnx_quantize/ort_config.json\n"
     ]
    }
   ],
   "source": [
    "# save model onnx quantized\n",
    "model_quantized_path = quantizer.quantize(\n",
    "    save_dir=save_directory+\"vit_gg_onnx_quantize\",\n",
    "    quantization_config=dqconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"/home/music/Desktop/measure_model/data/image_net/n01514668_cock.JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/music/.local/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "/home/music/.local/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# vit google onnx\n",
    "vit_gg_onnx_path = save_directory+\"vit_gg_onnx/model.onnx\"\n",
    "vit_gg_onnx = InferenceSession(vit_gg_onnx_path, providers=['CUDAExecutionProvider'])\n",
    "processor_vit_gg_onxx = AutoFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 325.0598907470703 ms\n"
     ]
    }
   ],
   "source": [
    "inputs = processor_vit_gg_onxx(images=img.convert(\"RGB\"), return_tensors=\"np\")\n",
    "st = time.time()\n",
    "outputs_onnx = vit_gg_onnx.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n",
    "delta = time.time() - st\n",
    "print(f\"runtime : {delta*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit google onnx quantized\n",
    "vit_gg_onnx_quantized_path = \"/home/music/Desktop/measure_model/models/vit_gg_onnx_quantize\"\n",
    "vit_gg_onnx_quantized_model_path = vit_gg_onnx_quantized_path+\"/model_quantized.onnx\"\n",
    "vit_gg_onnx_quantized = InferenceSession(vit_gg_onnx_quantized_model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "processor_vit_gg_onxx = AutoFeatureExtractor.from_pretrained(vit_gg_onnx_quantized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 307.5551986694336 ms\n"
     ]
    }
   ],
   "source": [
    "inputs = processor_vit_gg_onxx(images=img.convert(\"RGB\"), return_tensors=\"np\")\n",
    "st = time.time()\n",
    "outputs_onnx_quantize = vit_gg_onnx_quantized.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n",
    "delta = time.time() - st\n",
    "print(f\"runtime : {delta*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original ViT google\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "vit_gg = ViTModel.from_pretrained('/app/nfs_clientshare/mew/project/Similarity_model/weights/vitgg_lr2e05_ep3_loss0.0')\n",
    "processor_vit_gg = ViTImageProcessor.from_pretrained('/app/nfs_clientshare/mew/project/Similarity_model/weights/vitgg_lr2e05_ep3_loss0.0')\n",
    "vit_gg.eval().to('cpu')\n",
    "inputs = processor_vit_gg(images=img, return_tensors=\"pt\").to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime : 472.4152088165283 ms\n"
     ]
    }
   ],
   "source": [
    "start_time_torch = time.time()\n",
    "outputs = vit_gg(**inputs)\n",
    "delta_time_torch = time.time() - start_time_torch\n",
    "print(\"runtime :\", delta_time_torch*1000, \"ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
